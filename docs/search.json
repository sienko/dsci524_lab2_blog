[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Sienko’s Blog",
    "section": "",
    "text": "This is a blog website that I created using Quarto for one of my MDS labs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Communication and Argumentation in Data Science",
    "section": "",
    "text": "My Reflections on a Classification Task\n\n\n\n\n\n\nclassification\n\n\nprediction\n\n\nprobability\n\n\nscoring\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nSienko Ikhabi\n\n\n\n\n\n\n\n\n\n\n\n\nSienko’s Blog\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nSienko Ikhabi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/the-importance-of-predict_proba/index.html",
    "href": "posts/the-importance-of-predict_proba/index.html",
    "title": "My Reflections on a Classification Task",
    "section": "",
    "text": "In this post, I want to reflect on one of the labs we completed in the Feature and Model Selection course. Up until December 2024, most of our labs were structured and directed. However, in this particular lab, we were given the freedom to solve one of two machine learning problems, as well as to decide on the types of models we wanted to build. It was designed as a mini-project, with a one-week timeframe!\nThe mini-project provided an excellent opportunity to make independent decisions and experiment with the model types we had learned about. Its open-ended nature mirrored the kinds of choices data scientists make daily, allowing students to simulate real-world decision-making processes. For me, it became a valuable moment to reflect on everything we had covered in the MDS program, connect my learning to my prior work experience, and look ahead to what awaits after graduation.\nFor the mini-project, I chose to work on a classification problem using the Default of Credit Card Clients Dataset. This dataset consisted of 30,000 records and 24 features. The objective was to build a predictive model capable of estimating whether a bank client would default (fail to pay) their credit card bills.\nAs much as I might touch on some of the models and decisions I made during the mini-project, I want to focus this blog post on model evaluation and thinking about how the possible model you could be developing will be used in production (real life). And to be clear, my main focus will be on binary classification problems, with an emphasis on identifying the positive class.\nFor model evaluation, most machine learning courses start by introducing basic metrics like accuracy, precision, recall, F1-score, and the confusion matrix. These metrics are valuable as a starting point for evaluating and comparing candidate models. However, in practical applications, the most useful output from a classification model is often the soft classifications — the predicted probabilities (predict_proba) — rather than the hard classifications (binary 0/1 predictions).\nUsing these predicted probabilities, we can go beyond basic metrics by plotting and interpreting the Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves, along with their respective Area Under the Curve (AUC) values. These metrics provide deeper insights into each model’s ability to distinguish between classes based on computed probabilities.\nFurthermore, these curves allow us to explore different threshold scenarios for hard classifications. This helps us make informed decisions about where to set the classification threshold, rather than relying on the default 50%. By adjusting the threshold, we can optimize the model for specific use cases, such as minimizing false positives or maximizing recall and putting those decisions in the context of the problem at hand.\nIn theory, during the course we would define the setting of the threshold as asking ourselves whether we want to be more cautious or a bit more relaxed with the decision. However, in practical applications, the decision goes beyond intuitively understanding what “cautious” and “more relaxed” means in simple modeling terms (precision and recall). For instance in the problem of predicting clients who are likely to default on their credit card paymen, I would suggest talking to the business users who will use the model. You would need to discuss with them what they intend to do with your predictions. If for instance they say “We will send two extra email reminders regarding their bill to the clients with higher predicted probabilities compared to those with lower probabilities”, you then need to change this into tangible implications.\nFirst of all, how many clients would fall in this category? What would the cost of sending these reminders be? Is it one that is acceptable for the number captured by the threshold you want to set? What about the customer experience? Would such an action be considered to by the client (as a reminder) or would they see it as a bother (why are you reminding me again)? And because we know that we have True Positives and False Positives, which of those are a good compromise with the proposed threshold?\nBeyond the consideration for where to set the threshold, I also find that as much as the AUC and PR curve might be easy for data science practictioners to understand, some students might struggle to get an intuition of what is happening as the thresholds are adjusted. One approach I found useful was to create a side-by-side chart showing the ROC curve and the probability density distribution for the two classes under different thresholds as shown in Figure 1 below. This way, students can visualize how moving the threshold up or down affects the False Positive Rate and True Positive Rate on the ROC curve. What’s better, in Altair, it can be made as an interactive chart! You can visualize how the decision threshold moves in both charts as you change it.\n\n\n\n\n\n\nFigure 1: ROC Curve and Predicted Probabilities\n\n\n\nIn addition to the plot in Figure 1, I found plotting the probabilities for two different models can also help us compare how well the models are classifying the data. In Figure 2, I show the probability density distributions for a Logistic Regression model and an XGBoost model that I trained during the lab.\n\n\n\n\n\n\nFigure 2: Comparing Predicted Probabilities from two models\n\n\n\nWe can observe visually how well the XGBoost model does a better at separating the two classes. To me, such a visualization is intuitive and can help students better understand the numeric metrics (precision, recall and AUC).\nReferences: 1. Nawar, Anika (2019, September 30). Plotting ROC curve in Altair. https://nawarani.github.io/blog/blog-2.html"
  }
]