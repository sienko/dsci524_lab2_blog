---
title: "My Reflections on a Classification Task"
author: "Sienko Ikhabi"
date: "2025-01-18"
categories: [classification,prediction,probability,scoring]
---

In this post, I want to reflect on one of the labs we completed in the 
[Feature and Model Selection](https://github.com/UBC-MDS/DSCI_573_feat-model-select) course. 
Up until December 2024, most of our labs were structured and directed. 
However, in this particular lab, we were given the freedom to solve one of 
two machine learning problems, as well as to decide on the types of models 
we wanted to build. It was designed as a mini-project, with a one-week timeframe!

The mini-project provided an excellent opportunity to make independent decisions 
and experiment with the model types we had learned about. Its open-ended nature 
mirrored the kinds of choices data scientists make daily, allowing students to 
simulate real-world decision-making processes. For me, it became a valuable 
moment to reflect on everything we had covered in the 
[MDS](https://masterdatascience.ubc.ca/) program, connect 
my learning to my prior work experience, and look ahead to what awaits after graduation.

For the mini-project, I chose to work on a classification problem using the 
[Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). 
This dataset consisted of 30,000 records and 24 features. The objective was to build a 
predictive model capable of estimating whether a bank client would default (fail to pay) 
their credit card bills.

As much as I might touch on some of the models and decisions I made during the 
mini-project, I want to focus this blog post on model evaluation and thinking 
about how the possible model you could be developing will be used in production (real life).
And to be clear, my main focus will be on binary classification problems, with an emphasis
on identifying the positive class.

For model evaluation, most machine learning courses start by introducing 
basic metrics like accuracy, precision, recall, F1-score, and the confusion 
matrix. These metrics are valuable as a starting point for evaluating and 
comparing candidate models. However, in practical applications, the most 
useful output from a classification model is often the **soft classifications** — 
the predicted probabilities (`predict_proba`) — rather than 
the **hard classifications** (binary 0/1 predictions).

Using these predicted probabilities, we can go beyond basic metrics by 
plotting and interpreting the Receiver Operating Characteristic (ROC) and 
Precision-Recall (PR) curves, along with their respective Area Under the Curve (AUC) values. 
These metrics provide deeper insights into each model's ability to distinguish 
between classes based on computed probabilities.

Furthermore, these curves allow us to explore different threshold scenarios 
for **hard classifications**. This helps us make informed decisions about 
where to set the classification threshold, rather than relying on the default 50%. 
By adjusting the threshold, we can optimize the model for specific use cases, 
such as minimizing false positives or maximizing recall and putting those decisions
in the context of the problem at hand. 

In theory, during the course we would define the setting of the threshold as
asking ourselves whether we want to be more cautious or a bit more relaxed 
with the decision. However, in practical applications, the decision
goes beyond intuitively understanding what "cautious" and "more relaxed" means
in simple modeling terms (precision and recall). For instance in the problem 
of predicting clients who are likely to default on their credit card paymen, 
I would suggest talking to the business users who will use the model. You would
need to discuss with them what they intend to do with your predictions. If for instance
they say "We will send two extra email reminders regarding their bill to the clients 
with higher predicted probabilities compared to those with lower probabilities",
you then need to change this into tangible implications. 

First of all, how many clients would fall in this category? What would the cost of sending 
these reminders be? Is it one that is acceptable for the number captured by the threshold you
want to set? What about the **customer experience**? Would such an action be considered to 
by the client (as a reminder) or would they see it as a bother (why are you reminding me
again)? And because we know that we have True Positives and False Positives, which of those
are a good compromise with the proposed threshold?

Beyond the consideration for where to set the threshold, I also find that as much as the 
AUC and PR curve might be easy for data science practictioners to understand, 
some students might struggle to get an intuition of what is happening as the thresholds are
adjusted. One approach I found useful was to create a side-by-side
chart showing the ROC curve and the probability density distribution for the two classes
under different thresholds as shown in @fig-threshold below. This way, students can visualize 
how moving the threshold up or down affects the False Positive Rate 
and True Positive Rate on the ROC curve. What's better, in Altair, it can be made
as an interactive chart! You can visualize how the decision threshold 
moves in both charts as you change it.

![ROC Curve and Predicted Probabilities](threshold.png){#fig-threshold width=80%}

In addition to the plot in @fig-threshold, I found plotting the probabilities for two different
models can also help us compare how well the models are
classifying the data. In @fig-models, I show the probability density distributions for a
Logistic Regression model and an XGBoost model that I trained during the lab.

![Comparing Predicted Probabilities from two models](threshold.png){#fig-models width=80%}

We can observe visually how well the XGBoost model does a better at separating 
the two classes. To me, such a visualization is intuitive and can help students 
better understand the numeric metrics (precision, recall and AUC).


References:
1. Nawar, Anika (2019, September 30). Plotting ROC curve in Altair.  https://nawarani.github.io/blog/blog-2.html